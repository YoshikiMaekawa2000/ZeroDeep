{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークの学習"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数\n",
    "- ２乗和誤差(回帰問題)\n",
    "$$\n",
    "    E = \\frac{1}{2} \\sum_{k}(y_k - t_k)^2\n",
    "$$\n",
    "- 交差エントロピー誤差(分類問題)\n",
    "$$\n",
    "    E = -\\sum_{k}(t_k\\log(y_k))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "###２乗和誤差###\n",
    "def sum_squared_erro(y, t):\n",
    "    return 0.5*np.sum((y-t)**2)\n",
    "\n",
    "###交差エントロピー誤差###\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t=t.reshape(1, t.size)\n",
    "        y=y.reshape(1, y.size)\n",
    "        \n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))/y.shape[0]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ミニバッチ学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\"\"\"\n",
    "###データセットのダウンロード(本書のライブラリを写経するのが面倒だったのでkerasで代用)###\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "t_train = np.eye(N=10)[t_train.astype('int32').flatten()]   #どちらも同じ処理．one hot labelにしてる\n",
    "t_test = to_categorical(t_test)\n",
    "x_train.reshape(x_train.shape[0], 28*28)\n",
    "x_test.reshape(x_test.shape[0], 28*28)\n",
    "\n",
    "###ミニバッチを作成###\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数値微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###微分###\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h)-f(x-h))/(2*h)\n",
    "\n",
    "###勾配(各変数の偏微分を表したベクトル)###\n",
    "def numerical_gradient_1d(f, x):\n",
    "    h=1e-4\n",
    "    grad = np.zeros_like(x.size)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp = x[idx]\n",
    "\n",
    "        x[idx] = tmp+h\n",
    "        fx1 = f(x)\n",
    "\n",
    "        x[idx] = tmp-h\n",
    "        fx2 = f(x)\n",
    "    \n",
    "        grad[idx] = (fx1-fx2)/(2*h)\n",
    "        x[idx] = tmp\n",
    "\n",
    "    return grad\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "###勾配降下法###\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "\n",
    "    x=init_x\n",
    "    for i in range(step_num):\n",
    "        x -= lr * numerical_gradient_1d(f, x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import softmax, sigmoid\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)  #ガウス分布で初期化\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = softmax(self.predict(x))\n",
    "        return cross_entropy_error(y, t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = simpleNet()\n",
    "\n",
    "x=np.array([0.6, 0.9])\n",
    "p=net.predict(x)\n",
    "t=np.array([0, 0, 1])\n",
    "print(net.loss(x, t))\n",
    "\n",
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2層ニューラルネットワークの実装\n",
    "- ミニバッチを作成\n",
    "- 重みパラメータの勾配の算出\n",
    "- パラメータの更新\n",
    "- 繰り返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params ={}\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.rand(input_size, hidden_size)\n",
    "        self.params[\"b1\"] = np.zeros(hidden_size)\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.rand(hidden_size, output_size)\n",
    "        self.params[\"b2\"] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        z1 = sigmoid(np.dot(x, self.params[\"W1\"]) + self.params[\"b1\"])\n",
    "        z2 = softmax(np.dot(z1, self.params[\"W2\"]) + self.params[\"b2\"])\n",
    "        return z2\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy       \n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t) \n",
    "\n",
    "        grads = {}\n",
    "        grads[\"W1\"] = numerical_gradient(loss_W, self.params[\"W1\"])\n",
    "        grads[\"W2\"] = numerical_gradient(loss_W, self.params[\"W2\"])\n",
    "        grads[\"b1\"] = numerical_gradient(loss_W, self.params[\"b1\"])\n",
    "        grads[\"b2\"] = numerical_gradient(loss_W, self.params[\"b2\"])\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "t_train = np.eye(N=10)[t_train.astype('int32').flatten()]   #どちらも同じ処理．one hot labelにしてる\n",
    "t_test = to_categorical(t_test)\n",
    "x_train = x_train.astype(\"float32\")/255.\n",
    "\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "x_train = x_train.reshape([x_train.shape[0], 784])\n",
    "x_test = x_test.reshape([x_test.shape[0], 784])\n",
    "print(x_train.shape)\n",
    "iter_per_epoch = max(train_size/batch_size, 1)\n",
    "\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in (\"W1\", \"b1\", \"W2\", \"b2\"):\n",
    "        network.params[key] -= learning_rate*grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i%iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
